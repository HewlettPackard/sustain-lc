.. _evaluation:

Evaluation
==========

After running the benchmarks, you can compare the results by examining the output files in the ``results/`` directory. These files include detailed metrics on energy consumption, carbon footprint, and workload distribution across data centers. Use these metrics to assess the relative performance of different algorithms and configurations.

Evaluation Metrics
------------------

Green-DCC provides a range of evaluation metrics to assess the performance of the benchmarked algorithms:

- **Energy Consumption**: Total energy consumed by the data centers during the simulation.
- **Carbon Footprint**: Total carbon emissions generated by the data centers, calculated based on the energy mix and carbon intensity of the power grid.
- **Workload Distribution**: Efficiency of workload distribution across data centers, considering factors like latency, bandwidth cost, and data center utilization.

These metrics provide a comprehensive view of the performance of different algorithms and configurations, enabling you to identify the most effective strategies for sustainable data center management.

TensorBoard visualization
-------------------------

Benchmarking results can be monitored using TensorBoard. Green-DCC provides a custom callback implementation called ``CustomCallbacks`` (in ``utils/rllib_callbacks.py``) which can be used to track the performance of the model during training.

After starting the training process, view the results with:

.. code-block:: bash

   tensorboard --logdir=/results/

This will start a TensorBoard server; open your browser at ``http://localhost:6006`` to view the experiment visualizations.

Once TensorBoard is running, you can inspect metrics such as the average total energy with battery (``total_energy_with_battery``), the average CO2 footprint (``CO2_footprint_mean``), and the total load left (``load_left_mean``), as well as the model’s progress (e.g. ``episode_reward_mean``).

An example of the TensorBoard dashboard for Green-DCC experiments is shown below:

.. figure:: images/GreenDCC_tensorboard.png
   :alt: TensorBoard dashboard
   :align: center

Custom Metrics
--------------

To add new custom metrics to track during training with TensorBoard, modify the ``CustomCallbacks`` class:

- Create a new key in the ``episode.user_data`` dictionary in the ``on_episode_start`` method.
- Store or collect the desired metric’s value in the ``on_episode_step`` method.
- In the ``on_episode_end`` method (called at the end of each episode), save the final metric value into the ``episode.custom_metrics`` dictionary.

Once you have added your custom metric, select it from the TensorBoard dashboard’s metric dropdown to visualize it. Adding custom metrics in this way gives you greater flexibility and control over the training process, allowing you to track metrics specific to your use case and goals.
